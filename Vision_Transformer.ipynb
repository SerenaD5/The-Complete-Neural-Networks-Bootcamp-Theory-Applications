{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZqNoMSRSx9NP52mSl3XsG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "通过这段代码，我们将一个384x384的RGB图像划分为16x16的图像块，并通过线性投影将每个图像块转换为768维的向量。最终得到的张量形状为(1, 576, 768)，其中576是图像块的数量，768是每个图像块的向量维度。这个张量可以作为Transformer模型的输入。"
      ],
      "metadata": {
        "id": "EfxuhtUAzoYf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA8wcjwAzRkq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 定义图像大小和图像块大小\n",
        "image_size = 384  # 图像的高度和宽度都是384\n",
        "patch_size = 16  # 图像块的大小是16x16\n",
        "num_words = (image_size // patch_size) ** 2  # 计算总的图像块数目\n",
        "\n",
        "# 定义线性投影层（卷积层）\n",
        "patch_embed = nn.Conv2d(in_channels=3, out_channels=768, kernel_size=(16, 16), stride=(16, 16))\n",
        "\n",
        "# 创建一个模拟图像的随机张量\n",
        "x = torch.randn(1, 3, 384, 384)  # 批量大小为1，通道数为3，图像大小为384x384\n",
        "\n",
        "# 将图像通过图像块嵌入层\n",
        "x = patch_embed(x)\n",
        "\n",
        "# 查看输出张量的形状\n",
        "print(\"Shape after patch embedding:\", x.shape)  # 输出形状为 (1, 768, 24, 24)\n",
        "\n",
        "# 展平张量并转置\n",
        "x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "# 查看最终张量的形状\n",
        "print(\"Final shape:\", x.shape)  # 最终输出形状为 (1, 576, 768)\n",
        "\n",
        "# 打印最终张量的形状\n",
        "print(\"Final tensor shape:\", x.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 代码解释和图表含义\n",
        "\n",
        "这段代码展示了如何在PyTorch中实现Vision Transformer（ViT）的图像块嵌入（Patch Embedding）过程。以下是详细解释：\n",
        "\n",
        "### 代码步骤\n",
        "\n",
        "1. **导入库**：\n",
        "   ```python\n",
        "   import torch\n",
        "   import torch.nn as nn\n",
        "   ```\n",
        "\n",
        "2. **定义图像大小和图像块大小**：\n",
        "   ```python\n",
        "   image_size = 384  # 图像的高度和宽度都是384\n",
        "   patch_size = 16  # 图像块的大小是16x16\n",
        "   num_words = image_size // patch_size  # 计算总的图像块数目\n",
        "   ```\n",
        "\n",
        "3. **定义线性投影层（卷积层）**：\n",
        "   ```python\n",
        "   patch_embed = nn.Conv2d(in_channels=3, out_channels=768, kernel_size=(16, 16), stride=(16, 16))\n",
        "   ```\n",
        "   - `in_channels=3`：输入图像的通道数（RGB图像有3个通道）。\n",
        "   - `out_channels=768`：Transformer模型的维度。\n",
        "   - `kernel_size=(16, 16)`：卷积核的大小，与图像块大小一致。\n",
        "   - `stride=(16, 16)`：步幅大小，与图像块大小一致。\n",
        "\n",
        "4. **创建一个模拟图像的随机张量**：\n",
        "   ```python\n",
        "   x = torch.randn(1, 3, 384, 384)  # 批量大小为1，通道数为3，图像大小为384x384\n",
        "   ```\n",
        "\n",
        "5. **将图像通过图像块嵌入层**：\n",
        "   ```python\n",
        "   x = patch_embed(x)\n",
        "   ```\n",
        "\n",
        "6. **查看输出张量的形状**：\n",
        "   ```python\n",
        "   x.shape  # 输出形状为 (1, 768, 24, 24)\n",
        "   ```\n",
        "\n",
        "7. **展平张量**：\n",
        "   ```python\n",
        "   x = x.flatten(2).transpose(1, 2)\n",
        "   ```\n",
        "   - `x.flatten(2)`：从第2维开始展平张量，将 (1, 768, 24, 24) 转换为 (1, 768, 576)。\n",
        "   - `x.transpose(1, 2)`：转置张量，将 (1, 768, 576) 转换为 (1, 576, 768)。\n",
        "\n",
        "8. **查看最终张量的形状**：\n",
        "   ```python\n",
        "   x.shape  # 最终输出形状为 (1, 576, 768)\n",
        "   ```\n",
        "\n",
        "### 图表含义\n",
        "\n",
        "图表展示了Vision Transformer（ViT）的模型架构，具体如下：\n",
        "\n",
        "1. **图像块嵌入（Patch Embedding）**：\n",
        "   - 输入图像被划分为多个16x16的图像块。\n",
        "   - 每个图像块通过线性投影（卷积层）转换为一个向量。\n",
        "\n",
        "2. **位置嵌入（Position Embedding）**：\n",
        "   - 为每个图像块向量添加位置嵌入，以保留图像块在原始图像中的位置信息。\n",
        "\n",
        "3. **Transformer编码器（Transformer Encoder）**：\n",
        "   - 编码器由多个层组成，每层包含多头注意力机制（Multi-Head Attention）和前馈神经网络（MLP）。\n",
        "   - 编码器的输入是经过线性投影和位置嵌入处理后的图像块向量。\n",
        "\n",
        "4. **分类头（Classification Head）**：\n",
        "   - 在输入序列的开头添加一个额外的分类标记（Class Token）。\n",
        "   - Transformer编码器的输出通过一个多层感知机（MLP）进行分类，输出最终的类别标签（如鸟、球、车等）。\n",
        "\n",
        "通过这种架构，Vision Transformer能够有效地处理图像数据，并在图像识别任务中取得优异的表现。"
      ],
      "metadata": {
        "id": "6UmY646GzZ1J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WxjlB-x1zbwN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}