{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "the source for Aice - link: https://www.gutenberg.org/ebooks/11"
      ],
      "metadata": {
        "id": "FY9Ijj9QNdXA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y1ISxztSNWAd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn #用于神经网络模块\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm #用于梯度裁剪"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z_a-SecrNWAe"
      },
      "outputs": [],
      "source": [
        "class Dictionary(object): #定义Dictionary类，用于词汇表的构建和管理。\n",
        "    def __init__(self):\n",
        "        self.word2idx = {} #存储词到索引的映射，idx用于记录当前词汇表的大小。\n",
        "        self.idx2word = {} #存储索引到词的映射，idx用于记录当前词汇表的大小。\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):#用于向词汇表中添加新词。如果词不在词汇表中，则将其添加并更新索引\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __len__(self): #__len__方法返回词汇表的大小，即词汇表中词的数量\n",
        "        return len(self.word2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nzi9yROqNWAf"
      },
      "outputs": [],
      "source": [
        "class TextProcess(object): #定义TextProcess类，用于文本处理\n",
        "\n",
        "    def __init__(self):\n",
        "        self.dictionary = Dictionary() #初始化时创建一个Dictionary实例\n",
        "\n",
        "    def get_data(self, path, batch_size=20): #get_data方法用于从文件中读取数据并进行处理。首先打开文件并统计词的总数，同时将每个词添加到词汇表中\n",
        "        with open(path, 'r') as f:\n",
        "            tokens = 0\n",
        "            for line in f:\n",
        "              # 将行拆分为单词，并在行末添加<eos>标记\n",
        "                words = line.split() + ['<eos>']\n",
        "                tokens += len(words)\n",
        "                for word in words:\n",
        "                   # 将每个单词添加到词汇表中\n",
        "                    self.dictionary.add_word(word)\n",
        "        #Create a 1-D tensor that contains the index of all the words in the file\n",
        "        rep_tensor = torch.LongTensor(tokens) #创建一个长整型张量rep_tensor用于存储词的索引。\n",
        "        index = 0\n",
        "        with open(path, 'r') as f: #再次遍历文件，将每个词转换为索引并存储在张量中\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    rep_tensor[index] = self.dictionary.word2idx[word]\n",
        "                    index += 1\n",
        "        #Find out how many batches we need    计算需要的批次数量，去除多余的词，使张量的大小适应批次大小。最后将张量重塑为(batch_size, num_batches)的形状并返回\n",
        "        num_batches = rep_tensor.shape[0] // batch_size\n",
        "        #Remove the remainder (Filter out the ones that don't fit)\n",
        "        rep_tensor = rep_tensor[:num_batches*batch_size]\n",
        "        # return (batch_size,num_batches)\n",
        "        rep_tensor = rep_tensor.view(batch_size, -1)\n",
        "        return rep_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro4VnHckNWAf"
      },
      "outputs": [],
      "source": [
        "embed_size = 128    #Input features to the LSTM\n",
        "hidden_size = 1024  #Number of LSTM units\n",
        "num_layers = 1\n",
        "num_epochs = 20\n",
        "batch_size = 20\n",
        "timesteps = 30\n",
        "learning_rate = 0.002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xeS9YMRNWAg"
      },
      "outputs": [],
      "source": [
        "corpus = TextProcess()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xcw49OjNWAg"
      },
      "outputs": [],
      "source": [
        "rep_tensor = corpus.get_data('alice.txt', batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHbrJzT3NWAg"
      },
      "outputs": [],
      "source": [
        "#rep_tensor is the tensor that contains the index of all the words. Each row contains 1659 words by default\n",
        "print(rep_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy5jbGVdNWAg"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(corpus.dictionary)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLGnllUDNWAg"
      },
      "outputs": [],
      "source": [
        "num_batches = rep_tensor.shape[1] // timesteps\n",
        "print(num_batches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuMpPEB3NWAg"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(TextGenerator, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        # Perform Word Embedding\n",
        "        x = self.embed(x)\n",
        "        #Reshape the input tensor\n",
        "        #x = x.view(batch_size,timesteps,embed_size)\n",
        "        out, (h, c) = self.lstm(x, h)\n",
        "        # Reshape the output from (samples,timesteps,output_features) to a shape appropriate for the FC layer\n",
        "        # (batch_size*timesteps, hidden_size)\n",
        "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.linear(out)\n",
        "        return out, (h, c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlnSSCdHNWAh"
      },
      "outputs": [],
      "source": [
        "model = TextGenerator(vocab_size, embed_size, hidden_size, num_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4OBMhvHNWAh"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud-PQAUcNWAh"
      },
      "outputs": [],
      "source": [
        "def detach(states):\n",
        "    \"\"\"\n",
        "If we have a tensor z,'z.detach()' returns a tensor that shares the same storage\n",
        "as 'z', but with the computation history forgotten. It doesn't know anything\n",
        "about how it was computed. In other words, we have broken the tensor z away from its past history\n",
        "Here, we want to perform truncated Backpropagation\n",
        "TBPTT splits the 1,000-long sequence into 50 sequences (say) each of length 20 and treats each sequence of length 20 as\n",
        "a separate training case. This is a sensible approach that can work well in practice, but it is blind to temporal\n",
        "dependencies that span more than 20 timesteps.\n",
        "    \"\"\"\n",
        "    return [state.detach() for state in states]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RW507utNWAh"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # Set initial hidden and cell states\n",
        "    states = (torch.zeros(num_layers, batch_size, hidden_size),\n",
        "              torch.zeros(num_layers, batch_size, hidden_size))\n",
        "\n",
        "    for i in range(0, rep_tensor.size(1) - timesteps, timesteps):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = rep_tensor[:, i:i+timesteps]\n",
        "        targets = rep_tensor[:, (i+1):(i+1)+timesteps]\n",
        "\n",
        "        outputs,_ = model(inputs, states)\n",
        "        loss = loss_fn(outputs, targets.reshape(-1))\n",
        "\n",
        "        model.zero_grad()\n",
        "        loss.backward()\n",
        "        #Perform Gradient Clipping. clip_value (float or int) is the maximum allowed value of the gradients\n",
        "        #The gradients are clipped in the range [-clip_value, clip_value]. This is to prevent the exploding gradient problem\n",
        "        clip_grad_norm(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        step = (i+1) // timesteps\n",
        "        if step % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, num_epochs, loss.item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtZ-zWo7NWAh"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    with open('results.txt', 'w') as f:\n",
        "        # Set intial hidden ane cell states\n",
        "        state = (torch.zeros(num_layers, 1, hidden_size),\n",
        "                 torch.zeros(num_layers, 1, hidden_size))\n",
        "        # Select one word id randomly and convert it to shape (1,1)\n",
        "        input = torch.randint(0,vocab_size, (1,)).long().unsqueeze(1)\n",
        "\n",
        "        for i in range(500):\n",
        "            output, _ = model(input, state)\n",
        "            print(output.shape)\n",
        "            # Sample a word id from the exponential of the output\n",
        "            prob = output.exp()\n",
        "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "            print(word_id)\n",
        "            # Replace the input with sampled word id for the next time step\n",
        "            input.fill_(word_id)\n",
        "\n",
        "            # Write the results to file\n",
        "            word = corpus.dictionary.idx2word[word_id]\n",
        "            word = '\\n' if word == '<eos>' else word + ' '\n",
        "            f.write(word)\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print('Sampled [{}/{}] words and save to {}'.format(i+1, 500, 'results.txt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofEgeyg4NWAh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}