{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNkm02TL0wX_"
      },
      "source": [
        "### Gradient Accumulation\n",
        "\n",
        "- In many situations, we want to have a high batch size (desired batch size), however our GPU can only handle a specific batch size (tolerable batch size). One option is to have multiple GPUs and use distributed data training. But what if only one GPU is available? The solution is **gradient accumulation**.\n",
        "<br>\n",
        "<br>\n",
        "- Gradient accumulation (summation) is performing **multiple** backwards passes **before** updating the parameters. The goal is to have the same model parameters for multiple inputs (batches) and then update the model's parameters based on all these batches, instead of performing an update after every single batch.  So we run each torelarbale batch size individually with the same model parameters and calculate the gradients without updating the model. When the desired batch size is reached, we can then update the gradients.\n",
        "<br>\n",
        "<br>\n",
        "- Point of confusion for sudents. The **computational graph** is automatically destroyed when .backward() is called (unless retain_graph=True is specified), and **NOT** the gradients. The gradients are only reset when calling optimizer.zero_grad()\n",
        "<br>\n",
        "<br>\n",
        "- Let's implement that on a ResNet-101 using Google Colab GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuF6GRtwacxL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SZFbPs20_3j"
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.resnet101()\n",
        "num_iterations = 10\n",
        "xe = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfTbnNVL1Odo"
      },
      "outputs": [],
      "source": [
        "batch_size = 50   # this works, 100 does not\n",
        "for i in range(num_iterations):\n",
        "    inputs = torch.randn(batch_size,3,224,224)\n",
        "    labels = torch.LongTensor(batch_size).random_(0, 100)\n",
        "    loss = xe(model(inputs), labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    print(\"Done one batch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBtqQLBx4hzh"
      },
      "outputs": [],
      "source": [
        "desired_batch_size = 100\n",
        "tolerable_batch_size = 50\n",
        "accum_steps = desired_batch_size / tolerable_batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plu7srYS1cTl"
      },
      "outputs": [],
      "source": [
        "for i in range(num_iterations):\n",
        "    inputs = torch.randn(tolerable_batch_size,3,224,224)\n",
        "    labels = torch.LongTensor(tolerable_batch_size).random_(0, 100)\n",
        "    loss = xe(model(inputs), labels)\n",
        "    # The loss needs to be scaled to have the same significance over the whole dataset, because the mean should be taken across\n",
        "    # the whole dataset , which requires the loss to be divided by the number of batches. This is only the case if the loss\n",
        "    # returned is averaged. But if the loss returned is summed (for example: nn.BCELoss(reduction = 'sum')) then we do not need to do this\n",
        "    loss = loss / accum_steps\n",
        "    loss.backward()\n",
        "\n",
        "    if ((i + 1) % accum_steps == 0) or (i + 1 == num_iterations):\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(\"Done one batch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "# 导入必要的库\n",
        "# torch用于张量计算和自动微分\n",
        "# torchvision用于计算机视觉模型和数据集\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "\n",
        "# 加载预训练的ResNet-101模型\n",
        "# ResNet-101是一种深度残差网络，适用于图像分类任务\n",
        "model = torchvision.models.resnet101()\n",
        "\n",
        "# 设置迭代次数\n",
        "# num_iterations表示训练的总批次数\n",
        "num_iterations = 10\n",
        "\n",
        "# 定义交叉熵损失函数\n",
        "# nn.CrossEntropyLoss用于多分类任务的损失计算\n",
        "xe = nn.CrossEntropyLoss()\n",
        "\n",
        "# 定义Adam优化器\n",
        "# optimizer用于更新模型参数\n",
        "# lr表示学习率，5e-4是一个常见的初始学习率\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "\n",
        "# 设置批次大小\n",
        "# batch_size表示每次训练的样本数量\n",
        "batch_size = 50  # 这个大小可以在GPU上运行，100则不行\n",
        "\n",
        "# 进行训练循环\n",
        "for i in range(num_iterations):\n",
        "    # 生成随机输入数据\n",
        "    # inputs是一个形状为(batch_size, 3, 224, 224)的张量，表示一批图像数据\n",
        "    inputs = torch.randn(batch_size, 3, 224, 224)\n",
        "    \n",
        "    # 生成随机标签\n",
        "    # labels是一个形状为(batch_size)的张量，表示每个图像的分类标签\n",
        "    labels = torch.LongTensor(batch_size).random_(0, 100)\n",
        "    \n",
        "    # 计算损失\n",
        "    # 将输入数据传入模型，得到预测结果，并计算损失\n",
        "    loss = xe(model(inputs), labels)\n",
        "    \n",
        "    # 反向传播计算梯度\n",
        "    # loss.backward()计算损失相对于模型参数的梯度\n",
        "    loss.backward()\n",
        "    \n",
        "    # 更新模型参数\n",
        "    # optimizer.step()根据计算的梯度更新模型参数\n",
        "    optimizer.step()\n",
        "    \n",
        "    # 清零梯度\n",
        "    # optimizer.zero_grad()将梯度清零，以便下一次迭代\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # 打印当前批次完成信息\n",
        "    print(\"Done one batch\")\n",
        "\n",
        "# 设置期望的批次大小和可容忍的批次大小\n",
        "# desired_batch_size表示期望的总批次大小\n",
        "# tolerable_batch_size表示单次可容忍的批次大小\n",
        "desired_batch_size = 100\n",
        "tolerable_batch_size = 50\n",
        "\n",
        "# 计算累积步数\n",
        "# accum_steps表示需要累积多少个可容忍批次才能达到期望批次\n",
        "accum_steps = desired_batch_size / tolerable_batch_size\n",
        "\n",
        "# 进行累积梯度的训练循环\n",
        "for i in range(num_iterations):\n",
        "    # 生成随机输入数据\n",
        "    inputs = torch.randn(tolerable_batch_size, 3, 224, 224)\n",
        "    \n",
        "    # 生成随机标签\n",
        "    labels = torch.LongTensor(tolerable_batch_size).random_(0, 100)\n",
        "    \n",
        "    # 计算损失\n",
        "    loss = xe(model(inputs), labels)\n",
        "    \n",
        "    # 缩放损失\n",
        "    # 为了在整个数据集上具有相同的意义，需要将损失缩放\n",
        "    # 这里假设损失是平均的，如果损失是求和的（例如：nn.BCELoss(reduction='sum')），则不需要缩放\n",
        "    loss = loss / accum_steps\n",
        "    \n",
        "    # 反向传播计算梯度\n",
        "    loss.backward()\n",
        "\n",
        "    # 每累积一定步数后更新模型参数\n",
        "    if ((i + 1) % accum_steps == 0) or (i + 1 == num_iterations):\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        print(\"Done one batch\")\n",
        "```\n",
        "\n",
        "### 代码逐行解释\n",
        "\n",
        "1. `import torch`\n",
        "   - 导入PyTorch库，用于张量计算和自动微分。\n",
        "\n",
        "2. `import torchvision`\n",
        "   - 导入torchvision库，用于计算机视觉模型和数据集。\n",
        "\n",
        "3. `import torch.nn as nn`\n",
        "   - 导入PyTorch的神经网络模块，简化命名为nn。\n",
        "\n",
        "4. `model = torchvision.models.resnet101()`\n",
        "   - 加载预训练的ResNet-101模型，适用于图像分类任务。\n",
        "\n",
        "5. `num_iterations = 10`\n",
        "   - 设置训练的总批次数为10。\n",
        "\n",
        "6. `xe = nn.CrossEntropyLoss()`\n",
        "   - 定义交叉熵损失函数，用于多分类任务的损失计算。\n",
        "\n",
        "7. `optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)`\n",
        "   - 定义Adam优化器，用于更新模型参数，学习率设置为5e-4。\n",
        "\n",
        "8. `batch_size = 50`\n",
        "   - 设置每次训练的样本数量为50。\n",
        "\n",
        "9. `for i in range(num_iterations):`\n",
        "   - 开始训练循环，循环次数为num_iterations。\n",
        "\n",
        "10. `inputs = torch.randn(batch_size, 3, 224, 224)`\n",
        "    - 生成随机输入数据，形状为(batch_size, 3, 224, 224)。\n",
        "\n",
        "11. `labels = torch.LongTensor(batch_size).random_(0, 100)`\n",
        "    - 生成随机标签，形状为(batch_size)，标签范围为0到99。\n",
        "\n",
        "12. `loss = xe(model(inputs), labels)`\n",
        "    - 计算损失，将输入数据传入模型，得到预测结果，并计算损失。\n",
        "\n",
        "13. `loss.backward()`\n",
        "    - 反向传播计算梯度，计算损失相对于模型参数的梯度。\n",
        "\n",
        "14. `optimizer.step()`\n",
        "    - 更新模型参数，根据计算的梯度更新模型参数。\n",
        "\n",
        "15. `optimizer.zero_grad()`\n",
        "    - 清零梯度，将梯度清零，以便下一次迭代。\n",
        "\n",
        "16. `print(\"Done one batch\")`\n",
        "    - 打印当前批次完成信息。\n",
        "\n",
        "17. `desired_batch_size = 100`\n",
        "    - 设置期望的总批次大小为100。\n",
        "\n",
        "18. `tolerable_batch_size = 50`\n",
        "    - 设置单次可容忍的批次大小为50。\n",
        "\n",
        "19. `accum_steps = desired_batch_size / tolerable_batch_size`\n",
        "    - 计算累积步数，表示需要累积多少个可容忍批次才能达到期望批次。\n",
        "\n",
        "20. `for i in range(num_iterations):`\n",
        "    - 开始累积梯度的训练循环，循环次数为num_iterations。\n",
        "\n",
        "21. `inputs = torch.randn(tolerable_batch_size, 3, 224, 224)`\n",
        "    - 生成随机输入数据，形状为(tolerable_batch_size, 3, 224, 224)。\n",
        "\n",
        "22. `labels = torch.LongTensor(tolerable_batch_size).random_(0, 100)`\n",
        "    - 生成随机标签，形状为(tolerable_batch_size)，标签范围为0到99。\n",
        "\n",
        "23. `loss = xe(model(inputs), labels)`\n",
        "    - 计算损失，将输入数据传入模型，得到预测结果，并计算损失。\n",
        "\n",
        "24. `loss = loss / accum_steps`\n",
        "    - 缩放损失，为了在整个数据集上具有相同的意义，需要将损失缩放。\n",
        "\n",
        "25. `loss.backward()`\n",
        "    - 反向传播计算梯度，计算损失相对于模型参数的梯度。\n",
        "\n",
        "26. `if ((i + 1) % accum_steps == 0) or (i + 1 == num_iterations):`\n",
        "    - 每累积一定步数后更新模型参数。\n",
        "\n",
        "27. `optimizer.step()`\n",
        "    - 更新模型参数，根据计算的梯度更新模型参数。\n",
        "\n",
        "28. `optimizer.zero_grad()`\n",
        "    - 清零梯度，将梯度清零，以便下一次迭代。\n",
        "\n",
        "29. `print(\"Done one batch\")`\n",
        "    - 打印当前批次完成信息。\n",
        "\n",
        "### 代码分析与优化建议\n",
        "\n",
        "1. **梯度累积的实现**：\n",
        "   - 通过累积多个小批次的梯度来模拟大批次的训练，有效利用有限的GPU内存。\n",
        "   - 这种方法适用于单GPU训练时内存不足的情况。\n",
        "\n",
        "2. **损失缩放**：\n",
        "   - 在累积梯度时，需要对损失进行缩放，以确保梯度的平均值与大批次训练时一致。\n",
        "   - 如果使用的是求和损失函数（如`nn.BCELoss(reduction='sum')`），则不需要进行缩放。\n",
        "\n",
        "3. **性能优化**：\n",
        "   - 可以通过调整学习率、优化器参数等方式进一步优化训练性能。\n",
        "   - 考虑使用混合精度训练（FP16）来减少内存占用，提高计算效率。\n",
        "\n",
        "4. **代码可读性**：\n",
        "   - 添加详细的注释和文档，帮助理解代码逻辑和实现细节。\n",
        "   - 使用明确的变量命名，提高代码的可读性和维护性。\n",
        "\n",
        "5. **错误处理**：\n",
        "   - 添加异常处理机制，捕获并处理可能出现的错误，确保训练过程的稳定性。\n",
        "\n",
        "通过以上分析和优化建议，可以更好地理解和实现梯度累积，提高模型训练的效率和效果。"
      ],
      "metadata": {
        "id": "Y1mFhjMOarxr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_7eQqM22Af4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "gradient accum.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}